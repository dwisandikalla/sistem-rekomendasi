# -*- coding: utf-8 -*-
"""proyek sistem rekomendasi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17NYPnxAYqHhtkBwcvzpasJNRvv6c_-OW

# Sistem Rekomendasi
"""

import numpy as np
import pandas as pd
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""## Load Data"""

anime = pd.read_csv('https://raw.githubusercontent.com/dwisandikalla/sistem-rekomendasi/refs/heads/main/anime.csv')
anime

rating = pd.read_csv('https://raw.githubusercontent.com/dwisandikalla/sistem-rekomendasi/refs/heads/main/rating.csv')
rating

"""## Univariate EDA

Mengecek info untuk rating dan anime
"""

anime.info()
print('\n')
rating.info()

"""Mengecek nilai unik di anime dan missing  value"""

print('Nilai unik data anime: ')
display(anime.nunique(),'\n')
print('Missing value anime: ')
display(anime.isnull().sum(), '\n')
print('Missing value rating: ')
display(rating.isnull().sum())

print('Macam genre: ', anime.genre.unique())
print('Macam type: ', anime.type.unique())

"""Statistika deskriptif"""

print('Statistika deskriptif anime: ')
display(anime.describe(include='all'), '\n')
print('\n Statistika deskriptif rating: ')
display(rating.describe(include='all'))

"""Visualisasi distribusi rating (skala pengguna)"""

rating['rating'].value_counts().sort_index().plot(kind='bar', title='Distribusi Rating oleh Pengguna')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.xticks(rotation=45)
plt.grid()
plt.show()

"""Visualisasi distribusi rating rata-rata per anime"""

plt.figure(figsize=(10, 4))
sns.histplot(anime['rating'].dropna(), kde=True, bins=30)
plt.title('Distribusi Rating per Anime')
plt.xlabel('Rating')
plt.ylabel('Frekuensi')
plt.grid()
plt.show()

"""Genre Terbanyak"""

from collections import Counter
genre_list = anime['genre'].dropna().str.split(', ')
genre_flat = [genre for sublist in genre_list for genre in sublist]
genre_counts = pd.Series(Counter(genre_flat)).sort_values(ascending=False)
genre_counts.head(15).plot(kind='bar', title='15 Genre Terbanyak')
plt.ylabel('Jumlah Anime')
plt.grid()
plt.show()

"""Type Anime"""

anime['type'].value_counts().plot(kind='bar', title='Distribusi Tipe Anime')
plt.ylabel('Jumlah')
plt.grid()
plt.show()

"""## Data Preprocessing"""

anime.drop_duplicates(inplace=True)

"""Menghapus missing value"""

anime = anime.dropna()

anime.reset_index(drop=True, inplace=True)

"""Menghapus rating -1"""

rating = rating[rating['rating'] != -1]

"""Drop Duplikasi"""

rating.drop_duplicates(inplace=True)

rating.reset_index(drop=True, inplace=True)

# Pastikan hanya anime yang tersedia di data anime
rating = rating[rating['anime_id'].isin(anime['anime_id'])]

# Contoh: 10 rating per anime, jika memungkinkan
rating_per_anime = rating.groupby('anime_id').apply(lambda x: x.sample(min(len(x), 100), random_state=42)).reset_index(drop=True)

# Merge dataset anime dan rating
anime_rating = pd.merge(rating_per_anime, anime, on='anime_id')

# Final check dimensi
print("anime.shape:", anime.shape)
print("rating.shape:", rating.shape)
print("anime_rating.shape:", anime_rating.shape)

anime_rating.info()

anime_rating.isnull().sum()

anime_rating.head()

"""## Content Based Filtering"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

"""Ambil data yang diperlukan"""

data = anime_rating[['anime_id', 'name', 'genre']].drop_duplicates()
data['genre'] = data['genre'].fillna('')

"""TF-IDF Vectorizer"""

tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(data['genre'])

"""Hitung cosine similarity"""

cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['name'], columns=data['name'])

"""Fungsi rekomendasi"""

def anime_recommendations(nama_anime, similarity_data=cosine_sim_df, items=data[['name', 'genre']], k=5):
    index = similarity_data.loc[:, nama_anime].to_numpy().argpartition(range(-1, -k-1, -1))
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    closest = closest.drop(nama_anime, errors='ignore')
    return pd.DataFrame(closest).merge(items).head(k)

"""Contoh"""

anime_recommendations('Naruto')

"""## Collaborative Filtering"""

import tensorflow as tf
from tensorflow.keras import layers, regularizers, Model
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split

"""Memilih data yang akan digunakan"""

df = anime_rating[['user_id', 'anime_id', 'rating_x']].dropna()
df.rename(columns={'user_id': 'userID', 'anime_id': 'animeID'}, inplace=True)

"""Encoding user dan anime"""

user_ids = df['userID'].unique().tolist()
anime_ids = df['animeID'].unique().tolist()

user_to_encoded = {x: i for i, x in enumerate(user_ids)}
anime_to_encoded = {x: i for i, x in enumerate(anime_ids)}

df['user'] = df['userID'].map(user_to_encoded)
df['anime'] = df['animeID'].map(anime_to_encoded)

"""Normalisasi Rating"""

df['rating_x'] = df['rating_x'].astype('float32')
min_rating, max_rating = df['rating_x'].min(), df['rating_x'].max()
df['norm_rating'] = df['rating_x'].apply(lambda x: (x - min_rating) / (max_rating - min_rating))

"""Mengacak data"""

df = df.sample(frac=1, random_state=42)

"""Splitting data"""

x = df[['user', 'anime']].values
y = df['norm_rating'].values
x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)

"""Model Collaborative filtering"""

class RecommenderNet(Model):
    def __init__(self, num_users, num_anime, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.user_embedding = layers.Embedding(num_users, embedding_size,
                                               embeddings_initializer='he_normal',
                                               embeddings_regularizer=regularizers.l2(5e-4))
        self.anime_embedding = layers.Embedding(num_anime, embedding_size,
                                                embeddings_initializer='he_normal',
                                                embeddings_regularizer=regularizers.l2(5e-4))
        self.user_bias = layers.Embedding(num_users, 1)
        self.anime_bias = layers.Embedding(num_anime, 1)

        # Tambahan Dense + Dropout layer
        self.dense1 = layers.Dense(64, activation='relu')
        self.dropout = layers.Dropout(0.5)
        self.output_layer = layers.Dense(1, activation='sigmoid')  # karena rating distandardisasi ke 0-1

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        anime_vector = self.anime_embedding(inputs[:, 1])
        user_bias = self.user_bias(inputs[:, 0])
        anime_bias = self.anime_bias(inputs[:, 1])

        dot_product = tf.reduce_sum(user_vector * anime_vector, axis=1, keepdims=True)
        x = dot_product + user_bias + anime_bias
        x = self.dense1(x)
        x = self.dropout(x)
        return self.output_layer(x)

num_users = len(user_to_encoded)
num_anime = len(anime_to_encoded)

model = RecommenderNet(num_users, num_anime, embedding_size=50)
model.compile(
    loss='mean_squared_error',
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-5)

history = model.fit(
    x_train, y_train,
    batch_size=128,
    epochs=50,
    validation_data=(x_val, y_val),
    callbacks=[early_stop, lr_scheduler],
    verbose=1
)

"""Plot metrik evaluasi"""

import matplotlib.pyplot as plt

plt.plot(history.history['root_mean_squared_error'], label='Train RMSE')
plt.plot(history.history['val_root_mean_squared_error'], label='Validation RMSE')
plt.legend()
plt.title('Training vs Validation RMSE')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.show()

# Salin data anime
anime_copy = anime.copy()
df = anime_rating[['user_id', 'anime_id', 'rating_x']]

# Pilih 1 user secara acak
user_id = df.user_id.sample(1).iloc[0]
anime_rated_by_user = df[df.user_id == user_id]

# Ambil anime yang belum ditonton oleh user tersebut
anime_not_watched = anime_copy[~anime_copy['anime_id'].isin(anime_rated_by_user.anime_id.values)]['anime_id']
anime_not_watched = list(
    set(anime_not_watched)
    .intersection(set(anime_to_encoded.keys()))
)

encoded_to_anime = {v: k for k, v in anime_to_encoded.items()}


# Siapkan data untuk prediksi
anime_not_watched_encoded = [[anime_to_encoded.get(x)] for x in anime_not_watched]
user_encoded = user_to_encoded.get(user_id)
user_anime_array = np.hstack(
    ([[user_encoded]] * len(anime_not_watched_encoded), anime_not_watched_encoded)
)

# Prediksi rating
ratings = model.predict(user_anime_array).flatten()

# Ambil 10 rekomendasi terbaik
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_anime_ids = [
    encoded_to_anime.get(anime_not_watched_encoded[x][0]) for x in top_ratings_indices
]

# --- TAMPILKAN HASILNYA ---
print('Showing recommendations for user ID:', user_id)
print('=' * 30)

print('\nAnime with high ratings from this user')
print('-' * 30)

top_anime_user = (
    anime_rated_by_user.sort_values(
        by='rating_x',
        ascending=False
    ).head(5).anime_id.values
)

anime_df_rows = anime_copy[anime_copy['anime_id'].isin(top_anime_user)]
for row in anime_df_rows.itertuples():
    print(f"{row.name} | Genre: {row.genre if hasattr(row, 'genre') else 'N/A'}")

print('-' * 30)
print('Top 10 Anime Recommendations')
print('-' * 30)

recommended_anime = anime_copy[anime_copy['anime_id'].isin(recommended_anime_ids)]
for row in recommended_anime.itertuples():
    print(f"{row.name} | Genre: {row.genre if hasattr(row, 'genre') else 'N/A'}")